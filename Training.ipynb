{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matte\\myenv_RL\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from DQN import DQN_Agent\n",
    "from QR_DQN import QR_DQN_Agent\n",
    "from IQN import IQN_Agent\n",
    "from NAF import NAF_Agent\n",
    "from TD3 import TD3_Agent\n",
    "from PPO import PPO_Agent\n",
    "from SAC import SAC_Agent\n",
    "\n",
    "import optuna\n",
    "from optuna import create_study\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import PatientPruner, MedianPruner\n",
    "\n",
    "import json\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEFINITION OF THE TRIAL OBJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below there are defined all the optimizer object for each agent. Choose the one you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DQN_optim(trial):\n",
    "      # Define the space of hyperparameters to run the search for optimization\n",
    "      int_batch_size = trial.suggest_categorical(\"batch_size\", [64, 128])\n",
    "      int_gamma = trial.suggest_float(\"gamma\", 0.90, 0.99)\n",
    "      int_eps_start = trial.suggest_float(\"eps_start\", 0.95, 0.99)\n",
    "      int_eps_decay = trial.suggest_categorical(\"eps_decay\", [500, 750, 1000, 1250])\n",
    "      int_eps_end = trial.suggest_float(\"eps_end\", 0.025, 0.1)\n",
    "      int_tau = trial.suggest_float(\"tau\", 0.0025, 0.0075)\n",
    "      int_lr = trial.suggest_float(\"lr\", 1e-5, 1e-3)\n",
    "\n",
    "      # init the agent\n",
    "      model = DQN_Agent(ENV_NAME=\"LunarLander-v2\",\n",
    "            BATCH_SIZE=int(int_batch_size),\n",
    "            GAMMA=int_gamma,\n",
    "            EPS_START=int_eps_start,\n",
    "            EPS_DECAY=int(int_eps_decay),\n",
    "            EPS_END=int_eps_end,\n",
    "            TAU=int_tau,\n",
    "            LR=int_lr,\n",
    "            N_EPISODES=400,\n",
    "            PRINT_PLOT=True)\n",
    "      # run the training\n",
    "      model.training()\n",
    "      # return the loss to choose the hyper parameters\n",
    "      return  float(model.return_metric(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TD3_optim(trial):\n",
    "      # Define the space of hyperparameters to run the search for optimization\n",
    "      int_batch_size = trial.suggest_categorical(\"batch_size\", [64, 128])\n",
    "      int_gamma = trial.suggest_float(\"gamma\", 0.90, 0.99)\n",
    "      int_sd_noise = trial.suggest_float(\"sd_noise\", 0.3, 1)\n",
    "      int_sd_noise_decay = trial.suggest_float(\"sd_noise_decay\", 0.1, 0.99)\n",
    "      int_steps_decay_sd = trial.suggest_categorical(\"steps_decay_sd\", [5, 10, 15, 20])\n",
    "      int_cp_value = trial.suggest_float(\"cp_value\", 0.01, 0.2)\n",
    "      int_steps_update_policy = trial.suggest_categorical(\"steps_update_policy\", [2, 4, 6, 8, 10, 12, 14, 16, 18, 20])\n",
    "      int_tau = trial.suggest_float(\"tau\", 0.0025, 0.0075)\n",
    "      int_lr = trial.suggest_float(\"lr\", 1e-5, 1e-3)\n",
    "      int_repetition = trial.suggest_categorical(\"repetition\", [2, 4, 6, 8 ,10])\n",
    "\n",
    "      # init the agent\n",
    "      model = TD3_Agent(ENV_NAME=\"CarRacing-v3\",\n",
    "            BATCH_SIZE=int(int_batch_size),\n",
    "            GAMMA=int_gamma,\n",
    "            SD_NOISE=int_sd_noise,\n",
    "            SD_DECAY=int(int_sd_noise_decay),\n",
    "            STEPS_DECAY_SD=int_steps_decay_sd,\n",
    "            CP_VALUE=int_cp_value,\n",
    "            STEP_UPT_POLICY=int(int_steps_update_policy),\n",
    "            TAU=int_tau,\n",
    "            LR=int_lr,\n",
    "            REPETITION=int(int_repetition),\n",
    "            N_EPISODES=400,\n",
    "            PRINT_PLOT=False)\n",
    "      # run the training\n",
    "      model.training()\n",
    "      # return the loss to choose the hyper parameters\n",
    "      return  float(model.return_metric(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NAF_optim(trial):\n",
    "      # Define the space of hyperparameters to run the search for optimization\n",
    "      int_batch_size = trial.suggest_categorical(\"batch_size\", [64, 128, 256, 512])\n",
    "      int_gamma = trial.suggest_float(\"gamma\", 0.90, 0.99)\n",
    "      int_eps = trial.suggest_float(\"eps\", 0.5, 1)\n",
    "      int_eps_decay = trial.suggest_float(\"eps_decay\", 0.025, 0.1)\n",
    "      int_steps_decay = trial.suggest_categorical(\"steps_decay\", [5, 10, 15, 20, 40, 60])\n",
    "      int_tau = trial.suggest_float(\"tau\", 0.0025, 0.0075)\n",
    "      int_lr = trial.suggest_float(\"lr\", 1e-5, 1e-3)\n",
    "      int_repetition = trial.suggest_categorical(\"repetition\", [2, 4, 6, 8, 10])\n",
    "\n",
    "      # init the agent\n",
    "      model = NAF_Agent(ENV_NAME=\"CarRacing-v3\",\n",
    "            BATCH_SIZE=int(int_batch_size),\n",
    "            GAMMA=int_gamma,\n",
    "            EPSILON = int_eps,\n",
    "            EPSILON_DECAY=int_eps_decay,\n",
    "            STEPS_DECAY=int_steps_decay,\n",
    "            TAU=int_tau,\n",
    "            LR=int_lr,\n",
    "            REPETITION=int(int_repetition),\n",
    "            N_EPISODES=200,\n",
    "            PRINT_PLOT=False)\n",
    "      # run the training\n",
    "      model.training()\n",
    "      # return the loss to choose the hyper parameters\n",
    "      return  float(model.return_metric(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QR_DQN_optim(trial):\n",
    "      # Define the space of hyperparameters to run the search for optimization\n",
    "      int_batch_size = trial.suggest_categorical(\"batch_size\", [64, 128])\n",
    "      int_gamma = trial.suggest_float(\"gamma\", 0.90, 0.99)\n",
    "      int_eps_start = trial.suggest_float(\"eps_start\", 0.95, 0.99)\n",
    "      int_eps_decay = trial.suggest_categorical(\"eps_decay\", [500, 750, 1000, 1250])\n",
    "      int_eps_end = trial.suggest_float(\"eps_end\", 0.025, 0.1)\n",
    "      int_tau = trial.suggest_float(\"tau\", 0.0025, 0.0075)\n",
    "      int_lr = trial.suggest_float(\"lr\", 1e-5, 1e-3)\n",
    "      int_n_quantiles = trial.suggest_int(\"n_quantiles\", 40, 70)\n",
    "\n",
    "      # init the agent\n",
    "      model = QR_DQN_Agent(ENV_NAME=\"LunarLander-v2\",\n",
    "            BATCH_SIZE=int(int_batch_size),\n",
    "            GAMMA=int_gamma,\n",
    "            EPS_START=int_eps_start,\n",
    "            EPS_DECAY=int(int_eps_decay),\n",
    "            EPS_END=int_eps_end,\n",
    "            TAU=int_tau,\n",
    "            LR=int_lr,\n",
    "            N_QUANTILES=int_n_quantiles,\n",
    "            N_EPISODES=400,\n",
    "            PRINT_PLOT=False)\n",
    "      # run the training\n",
    "      model.training()\n",
    "      # return the loss to choose the hyper parameters\n",
    "      return  float(model.return_metric(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SAC_optim(trial):\n",
    "      # Define the space of hyperparameters to run the search for optimization\n",
    "      int_batch_size = trial.suggest_categorical(\"batch_size\", [64, 128, 256, 512])\n",
    "      int_gamma = trial.suggest_float(\"gamma\", 0.90, 0.99)\n",
    "      int_entropy_param = trial.suggest_float(\"entropy\", 0.05, 0.2)\n",
    "      int_epochs = trial.suggest_int(\"epochs\", 4, 30)\n",
    "      int_steps_update = trial.suggest_categorical(\"steps_update\", [10, 20, 30])\n",
    "      int_tau = trial.suggest_float(\"tau\", 0.0025, 0.0075)\n",
    "      int_lr = trial.suggest_float(\"lr\", 1e-5, 1e-3)\n",
    "      int_repetition = trial.suggest_categorical(\"repetition\", [2, 4, 6, 8 ,10])\n",
    "\n",
    "      # init the agent\n",
    "      model = SAC_Agent(ENV_NAME=\"CarRacing-v3\",\n",
    "            BATCH_SIZE=int(int_batch_size),\n",
    "            GAMMA=int_gamma,\n",
    "            ENTROPY_PARAM=int_entropy_param,\n",
    "            K_EPOCHS=int_epochs,\n",
    "            STEPS_UPDATE=int_steps_update,\n",
    "            TAU=int_tau,\n",
    "            LR=int_lr,\n",
    "            REPETITION=int(int_repetition),\n",
    "            N_EPISODES=400,\n",
    "            PRINT_PLOT=False)\n",
    "      # run the training\n",
    "      model.training()\n",
    "      # return the loss to choose the hyper parameters\n",
    "      return  float(model.return_metric(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IQN_optim(trial):\n",
    "      # Define the space of hyperparameters to run the search for optimization\n",
    "      int_batch_size = trial.suggest_categorical(\"batch_size\", [64, 128, 256, 512])\n",
    "      int_gamma = trial.suggest_float(\"gamma\", 0.90, 0.99)\n",
    "      int_eps_start = trial.suggest_float(\"eps_start\", 0.95, 0.99)\n",
    "      int_eps_decay = trial.suggest_categorical(\"eps_decay\", [500, 750, 1000, 1250])\n",
    "      int_eps_end = trial.suggest_float(\"eps_end\", 0.025, 0.1)\n",
    "      int_tau = trial.suggest_float(\"tau\", 0.0025, 0.0075)\n",
    "      int_lr = trial.suggest_float(\"lr\", 1e-5, 1e-3)\n",
    "      int_sub_agents = trial.suggest_int(\"sub_agents\", 2, 10)\n",
    "\n",
    "      # init the agent\n",
    "      model = IQN_Agent(ENV_NAME=\"LunarLander-v2\",\n",
    "            BATCH_SIZE=int(int_batch_size),\n",
    "            GAMMA=int_gamma,\n",
    "            EPS_START=int_eps_start,\n",
    "            EPS_DECAY=int(int_eps_decay),\n",
    "            EPS_END=int_eps_end,\n",
    "            TAU=int_tau,\n",
    "            LR=int_lr,\n",
    "            SUB_AGENTS=int_sub_agents\n",
    "            N_EPISODES=400,\n",
    "            PRINT_PLOT=False)\n",
    "      # run the training\n",
    "      model.training()\n",
    "      # return the loss to choose the hyper parameters\n",
    "      return  float(model.return_metric(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PPO_optim(trial):\n",
    "      # Define the space of hyperparameters to run the search for optimization\n",
    "      int_batch_size = trial.suggest_categorical(\"batch_size\", [64, 128, 256, 512])\n",
    "      int_num_batch_max = trial.suggest_int(\"max_n_batch\", 2, 10)\n",
    "      int_gamma = trial.suggest_float(\"gamma\", 0.90, 0.99)\n",
    "      int_trunc_param = trial.suggest_int(\"trunc_param\", 2, 6)\n",
    "      int_lambda = trial.suggest_float(\"lambda\", 0.01, 0.99)\n",
    "      int_max_len_traj = int.suggest_int(\"len_max_traj\", 500, 2000)\n",
    "      int_n_actors = trial.suggest_int(\"n_actors\", 2, 6)\n",
    "      int_epochs = trial.suggest_int(\"epochs\", 4, 30)\n",
    "      int_clip_value = trial.suggest_float(\"clip_value\", 0.01, 0.3)\n",
    "      int_entropy = trial.suggest_float(\"entropy_coef\", 0.1, 0.6)\n",
    "      int_lr = trial.suggest_float(\"lr\", 1e-5, 1e-3)\n",
    "      int_repetition = trial.suggest_categorical(\"repetition\", [2, 4, 6, 8 ,10])\n",
    "\n",
    "      # init the agent\n",
    "      model = PPO_Agent(ENV_NAME=\"CarRacing-v3\",\n",
    "            BATCH_SIZE=int(int_batch_size),\n",
    "            NUM_BATCH_MAX=int_num_batch_max,\n",
    "            GAMMA=int_gamma,\n",
    "            TRUNC_PARAM=int_trunc_param,\n",
    "            LAMBDA=int_lambda,\n",
    "            MAX_LEN_TRAJ=int_max_len_traj,\n",
    "            N_ACTORS=int_n_actors,\n",
    "            K_EPOCHS=int_epochs,\n",
    "            CLIP_VALUE=int_clip_value,\n",
    "            COEF_H=int_entropy,\n",
    "            LR=int_lr,\n",
    "            REPETITION=int(int_repetition),\n",
    "            N_EPISODES=400,\n",
    "            PRINT_PLOT=False)\n",
    "      # run the training\n",
    "      model.training()\n",
    "      # return the loss to choose the hyper parameters\n",
    "      return  float(model.return_metric(4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INITIALIZATION OF THE HYPERPARAMETER OPTIMIZER AND RUN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, replace DQN_optim with the model you selected above (so replace it with the correct optim)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matte\\AppData\\Local\\Temp\\ipykernel_1840\\3610508889.py:1: ExperimentalWarning: PatientPruner is experimental (supported from v2.8.0). The interface can change in the future.\n",
      "  study = create_study(direction=\"maximize\", sampler=TPESampler(), pruner=PatientPruner(MedianPruner(), patience=3))\n",
      "[I 2024-12-01 15:33:08,088] A new study created in memory with name: no-name-ac1335e0-974b-454b-82ce-16c04f21d430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video c:\\Users\\matte\\Documents\\REI LEA\\Progetto Esame\\Deep-RL-Algo\\videos\\rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video c:\\Users\\matte\\Documents\\REI LEA\\Progetto Esame\\Deep-RL-Algo\\videos\\rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2024-12-01 15:33:11,096] Trial 0 failed with parameters: {'batch_size': 128, 'gamma': 0.964727910854953, 'eps_start': 0.9646448664481089, 'eps_decay': 1000, 'eps_end': 0.06021138714889231, 'tau': 0.006188506403533536, 'lr': 0.00030694694214773233, 'n_quantiles': 57} because of the following error: RuntimeError('shape mismatch: value tensor of shape [127, 51] cannot be broadcast to indexing result of shape [127, 57]').\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\matte\\myenv_RL\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\matte\\AppData\\Local\\Temp\\ipykernel_1840\\138376655.py\", line 25, in QR_DQN_optim\n",
      "    model.training()\n",
      "  File \"c:\\Users\\matte\\Documents\\REI LEA\\Progetto Esame\\Deep-RL-Algo\\QR_DQN.py\", line 144, in training\n",
      "    self.optimize_model()\n",
      "  File \"c:\\Users\\matte\\Documents\\REI LEA\\Progetto Esame\\Deep-RL-Algo\\QR_DQN.py\", line 103, in optimize_model\n",
      "    next_state_values[non_final_mask] = torch.gather(target_output_quantiles, 1, target_output_quantiles.mean(dim = 2).max(dim = 1).indices.unsqueeze(1).unsqueeze(1).expand(-1, -1, 51)).squeeze(1)\n",
      "    ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^\n",
      "RuntimeError: shape mismatch: value tensor of shape [127, 51] cannot be broadcast to indexing result of shape [127, 57]\n",
      "[W 2024-12-01 15:33:11,110] Trial 0 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready c:\\Users\\matte\\Documents\\REI LEA\\Progetto Esame\\Deep-RL-Algo\\videos\\rl-video-episode-0.mp4\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape mismatch: value tensor of shape [127, 51] cannot be broadcast to indexing result of shape [127, 57]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m study \u001b[38;5;241m=\u001b[39m create_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m, sampler\u001b[38;5;241m=\u001b[39mTPESampler(), pruner\u001b[38;5;241m=\u001b[39mPatientPruner(MedianPruner(), patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m----> 2\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQR_DQN_optim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\matte\\myenv_RL\\Lib\\site-packages\\optuna\\study\\study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\matte\\myenv_RL\\Lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\matte\\myenv_RL\\Lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\matte\\myenv_RL\\Lib\\site-packages\\optuna\\study\\_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    247\u001b[0m ):\n\u001b[1;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\matte\\myenv_RL\\Lib\\site-packages\\optuna\\study\\_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[7], line 25\u001b[0m, in \u001b[0;36mQR_DQN_optim\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     13\u001b[0m model \u001b[38;5;241m=\u001b[39m QR_DQN_Agent(ENV_NAME\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLunarLander-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     14\u001b[0m       BATCH_SIZE\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(int_batch_size),\n\u001b[0;32m     15\u001b[0m       GAMMA\u001b[38;5;241m=\u001b[39mint_gamma,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m       N_EPISODES\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m400\u001b[39m,\n\u001b[0;32m     23\u001b[0m       PRINT_PLOT\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# run the training\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# return the loss to choose the hyper parameters\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m  \u001b[38;5;28mfloat\u001b[39m(model\u001b[38;5;241m.\u001b[39mreturn_metric(\u001b[38;5;241m4\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\matte\\Documents\\REI LEA\\Progetto Esame\\Deep-RL-Algo\\QR_DQN.py:144\u001b[0m, in \u001b[0;36mQR_DQN_Agent.training\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    141\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# Perform one step of the optimization (on the policy network)\u001b[39;00m\n\u001b[1;32m--> 144\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;66;03m# Soft update of the target network's weights\u001b[39;00m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;66;03m# Polyak Average\u001b[39;00m\n\u001b[0;32m    148\u001b[0m target_net_state_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_net\u001b[38;5;241m.\u001b[39mstate_dict()\n",
      "File \u001b[1;32mc:\\Users\\matte\\Documents\\REI LEA\\Progetto Esame\\Deep-RL-Algo\\QR_DQN.py:103\u001b[0m, in \u001b[0;36mQR_DQN_Agent.optimize_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;66;03m# select Q(s',a) for quanile version with highest mean \u001b[39;00m\n\u001b[0;32m    102\u001b[0m     target_output_quantiles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_net(non_final_next_states)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_quantiles) \n\u001b[1;32m--> 103\u001b[0m     \u001b[43mnext_state_values\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnon_final_mask\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mgather(target_output_quantiles, \u001b[38;5;241m1\u001b[39m, target_output_quantiles\u001b[38;5;241m.\u001b[39mmean(dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmax(dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m51\u001b[39m))\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m) \n\u001b[0;32m    105\u001b[0m expected_state_action_values \u001b[38;5;241m=\u001b[39m (next_state_values \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma) \u001b[38;5;241m+\u001b[39m reward_batch\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# Compute the expected Q values plus the reward. To speed up compuation steps\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# Compute quantile Huber loss --> step of TD(0)\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape mismatch: value tensor of shape [127, 51] cannot be broadcast to indexing result of shape [127, 57]"
     ]
    }
   ],
   "source": [
    "study = create_study(direction=\"maximize\", sampler=TPESampler(), pruner=PatientPruner(MedianPruner(), patience=3))\n",
    "study.optimize(QR_DQN_optim, n_trials=60, n_jobs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GET HYPERPARAMETERS AND RE-TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_param = study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 128,\n",
       " 'gamma': 0.9870468671594415,\n",
       " 'eps_start': 0.9689218973923628,\n",
       " 'eps_decay': 1000,\n",
       " 'eps_end': 0.0841014572098897,\n",
       " 'tau': 0.003998980031293827,\n",
       " 'lr': 0.00037088923945978876}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matte\\myenv_RL\\Lib\\site-packages\\gymnasium\\wrappers\\record_video.py:87: UserWarning: \u001b[33mWARN: Overwriting existing videos at c:\\Users\\matte\\Documents\\REI LEA\\Progetto Esame\\Deep-RL-Algo\\videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "model = DQN_Agent(ENV_NAME=\"LunarLander-v2\",\n",
    "                  BATCH_SIZE=best_param[\"batch_size\"],\n",
    "                  GAMMA=best_param[\"gamma\"],\n",
    "                  EPS_START=best_param[\"eps_start\"],\n",
    "                  EPS_DECAY=best_param[\"eps_decay\"],\n",
    "                  EPS_END=best_param[\"eps_end\"],\n",
    "                  TAU=best_param[\"tau\"],\n",
    "                  LR=best_param[\"lr\"],\n",
    "                  N_EPISODES=600,\n",
    "                  PRINT_PLOT=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TD3_Agent(ENV_NAME=\"CarRacing-v3\",\n",
    "                  BATCH_SIZE=best_param[\"batch_size\"],\n",
    "                  GAMMA=best_param[\"gamma\"],\n",
    "                  SD_NOISE=best_param[\"sd_noise\"],\n",
    "                  SD_DECAY=best_param[\"sd_noise_decay\"],\n",
    "                  STEPS_DECAY_SD=best_param[\"steps_decay_sd\"],\n",
    "                  CP_VALUE=best_param[\"cp_value\"],\n",
    "                  STEP_UPT_POLICY=best_param[\"steps_update_policy\"],\n",
    "                  TAU=best_param[\"tau\"],\n",
    "                  LR=best_param[\"lr\"],\n",
    "                  REPETITION=best_param[\"repetition\"],\n",
    "                  N_EPISODES=400,\n",
    "                  PRINT_PLOT=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NAF_Agent(ENV_NAME=\"CarRacing-v3\",\n",
    "                  BATCH_SIZE=best_param[\"batch_size\"],\n",
    "                  GAMMA=best_param[\"gamma\"],\n",
    "                  EPSILON = best_param[\"eps\"],\n",
    "                  EPSILON_DECAY=best_param[\"eps_decay\"],\n",
    "                  STEPS_DECAY=best_param[\"steps_decay\"],\n",
    "                  TAU=best_param[\"tau\"],\n",
    "                  LR=best_param[\"lr\"],\n",
    "                  REPETITION=best_param[\"repetition\"],\n",
    "                  N_EPISODES=200,\n",
    "                  PRINT_PLOT=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = QR_DQN_Agent(ENV_NAME=\"LunarLander-v2\",\n",
    "                     BATCH_SIZE=best_param[\"batch_size\"],\n",
    "                     GAMMA=best_param[\"gamma\"],\n",
    "                     EPS_START=best_param[\"eps_start\"],\n",
    "                     EPS_DECAY=best_param[\"eps_decay\"],\n",
    "                     EPS_END=best_param[\"eps_end\"],\n",
    "                     TAU=best_param[\"tau\"],\n",
    "                     LR=best_param[\"lr\"],\n",
    "                     N_QUANTILES=best_param[\"n_quantiles\"],\n",
    "                     N_EPISODES=400,\n",
    "                     PRINT_PLOT=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SAC_Agent(ENV_NAME=\"CarRacing-v3\",\n",
    "                  BATCH_SIZE=best_param[\"batch_size\"],\n",
    "                  GAMMA=best_param[\"gamma\"],\n",
    "                  ENTROPY_PARAM=best_param[\"entropy\"],\n",
    "                  K_EPOCHS=best_param[\"epochs\"],\n",
    "                  STEPS_UPDATE=best_param[\"steps_update\"],\n",
    "                  TAU=best_param[\"tau\"],\n",
    "                  LR=best_param[\"lr\"],\n",
    "                  REPETITION=best_param[\"repetition\"],\n",
    "                  N_EPISODES=400,\n",
    "                  PRINT_PLOT=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = IQN_Agent(ENV_NAME=\"LunarLander-v2\",\n",
    "                     BATCH_SIZE=best_param[\"batch_size\"],\n",
    "                     GAMMA=best_param[\"gamma\"],\n",
    "                     EPS_START=best_param[\"eps_start\"],\n",
    "                     EPS_DECAY=best_param[\"eps_decay\"],\n",
    "                     EPS_END=best_param[\"eps_end\"],\n",
    "                     TAU=best_param[\"tau\"],\n",
    "                     LR=best_param[\"lr\"],\n",
    "                     SUB_AGENTS=best_param[\"sub_agents\"],\n",
    "                     N_EPISODES=400,\n",
    "                     PRINT_PLOT=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO_Agent(ENV_NAME=\"CarRacing-v3\",            \n",
    "                 BATCH_SIZE=best_param[\"batch_size\"],\n",
    "                 NUM_BATCH_MAX=best_param[\"max_n_batch\"],\n",
    "                 GAMMA=best_param[\"gamma\"],\n",
    "                 TRUNC_PARAM=best_param[\"trunc_param\"],\n",
    "                 LAMBDA=best_param[\"lambda\"],\n",
    "                 MAX_LEN_TRAJ=best_param[\"len_max_traj\"],\n",
    "                 N_ACTORS=best_param[\"n_actors\"],\n",
    "                 K_EPOCHS=best_param[\"epochs\"],\n",
    "                 CLIP_VALUE=best_param[\"clip_value\"],\n",
    "                 COEF_H=best_param[\"entropy_coef\"],\n",
    "                 LR=best_param[\"lr\"],\n",
    "                 REPETITION=best_param[\"repetition\"],\n",
    "                 N_EPISODES=400,\n",
    "                 PRINT_PLOT=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAVE BEST COMBINATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH1 = \"DQN_best_hyperparameters.json\"\n",
    "FILE_PATH2 = \"DQN_best_parameters.pt\"\n",
    "\n",
    "with open(FILE_PATH1, 'w') as json_file:\n",
    "    json.dump(best_param, json_file, indent=4)\n",
    "\n",
    "torch.save(model.return_weights(), FILE_PATH2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
